{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate accelerate peft trl\n",
    "# !pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d5cd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./lora_results_2_4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup run parameters\n",
    "lora_r = 2\n",
    "lora_alpha = 4\n",
    "use_QLoRA = False\n",
    "if use_QLoRA:\n",
    "    output_dir = './qlora_results_'+str(lora_r)+\"_\"+str(lora_alpha)\n",
    "else:\n",
    "    output_dir = './lora_results_'+str(lora_r)+\"_\"+str(lora_alpha)\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f55dc3-1cc3-44e1-804b-7bcf9c80d902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msarthak99\u001b[0m (\u001b[33mhpml99\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LoRA_2_4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# wandb.login(relogin=True)\n",
    "wandb.login()\n",
    "\n",
    "if use_QLoRA:\n",
    "    run_name = \"QLoRA_\"+str(lora_r)+\"_\"+str(lora_alpha)\n",
    "else:\n",
    "    run_name = \"LoRA_\"+str(lora_r)+\"_\"+str(lora_alpha)\n",
    "\n",
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf6c342-56dd-428c-b08b-c51d980b4c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/sg8304/project/LoRA_2_4/wandb/run-20240512_005032-3t9v2yy4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hpml99/lora_and_qlora_v2/runs/3t9v2yy4' target=\"_blank\">LoRA_2_4</a></strong> to <a href='https://wandb.ai/hpml99/lora_and_qlora_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hpml99/lora_and_qlora_v2' target=\"_blank\">https://wandb.ai/hpml99/lora_and_qlora_v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hpml99/lora_and_qlora_v2/runs/3t9v2yy4' target=\"_blank\">https://wandb.ai/hpml99/lora_and_qlora_v2/runs/3t9v2yy4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hpml99/lora_and_qlora_v2/runs/3t9v2yy4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1501133a41d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"lora_and_qlora_v2\",\n",
    "    name=run_name,\n",
    "#     tags=[\"baseline\", \"high-lr\"],\n",
    "#     group=\"bert\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "base_model = 'roberta-base'\n",
    "\n",
    "dataset = load_dataset('ag_news')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding=True)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
    "train_dataset=tokenized_dataset['train']\n",
    "eval_dataset=tokenized_dataset['test'].shard(num_shards=2, index=0)\n",
    "test_dataset=tokenized_dataset['test'].shard(num_shards=2, index=1)\n",
    "\n",
    "\n",
    "# Extract the number of classess and their names\n",
    "num_labels = dataset['train'].features['label'].num_classes\n",
    "class_names = dataset[\"train\"].features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our classifier.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a73755-61b2-4379-a9e6-74399672cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_QLoRA:\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    #     llm_int8_skip_modules=[\"out_lin\", \"lin1\", \"lin2\", \"word_embeddings\", \"position_embeddings\", \"pre_classifier\", \"classifier\"],\n",
    "        # llm_int8_skip_modules=[\"pre_classifier\", \"classifier\"],\n",
    "        llm_int8_skip_modules=['classifier'],\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "if use_QLoRA:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model,\n",
    "        # load_in_4bit=True,\n",
    "        quantization_config=nf4_config,\n",
    "        id2label=id2label)\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652452e3",
   "metadata": {},
   "source": [
    "## Setup PEFT Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT Config\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias = 'none',\n",
    "    target_modules = ['query', 'value'],\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "882b77ab-91e8-46c6-9ccd-3177487afbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# if use_QLoRA:\n",
    "#     model = prepare_model_for_kbit_training(model)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec2739d-76b6-4fde-91c2-0fc49e1884b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=2, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=2, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da45f85c-b016-4c49-8808-6eafa7cb5d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model\n",
      "trainable params: 667,396 || all params: 125,316,104 || trainable%: 0.5326\n"
     ]
    }
   ],
   "source": [
    "print('PEFT Model')\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea05443-7f23-4972-bb16-1afaf174dfef",
   "metadata": {},
   "source": [
    "## For tracking GPU Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "711f5a74-6f35-430d-8176-db976993e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 270 MB.\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768b4917-65de-4e55-ae7f-698e287535d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same Training args for all models\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy='steps',\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    use_cpu=False,\n",
    "    dataloader_num_workers=1,\n",
    "    # max_steps=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=False,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':True}\n",
    ")\n",
    "\n",
    "def get_trainer(model):\n",
    "      return  Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          compute_metrics=compute_metrics,\n",
    "          train_dataset=train_dataset,\n",
    "          eval_dataset=eval_dataset,\n",
    "          data_collator=data_collator,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98d9d57d-b57f-4acc-80fb-fc5443e75515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 34:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.771800</td>\n",
       "      <td>0.324230</td>\n",
       "      <td>0.897632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.291100</td>\n",
       "      <td>0.314905</td>\n",
       "      <td>0.901579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.312474</td>\n",
       "      <td>0.900526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>0.296613</td>\n",
       "      <td>0.910789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.289522</td>\n",
       "      <td>0.906842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>0.277634</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.276722</td>\n",
       "      <td>0.912632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.271699</td>\n",
       "      <td>0.909474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.270384</td>\n",
       "      <td>0.913947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.270103</td>\n",
       "      <td>0.912105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.262797</td>\n",
       "      <td>0.915263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.253200</td>\n",
       "      <td>0.263813</td>\n",
       "      <td>0.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.263670</td>\n",
       "      <td>0.915789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.252400</td>\n",
       "      <td>0.259281</td>\n",
       "      <td>0.915789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.259479</td>\n",
       "      <td>0.915263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_lora_finetuning_trainer = get_trainer(model)\n",
    "\n",
    "result = peft_lora_finetuning_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcc1b3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▂▆▅▆▇▆▇▇█████</td></tr><tr><td>eval/loss</td><td>█▇▇▅▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▄▆▆▃▄▃▅▆▇▂▅▅▅█</td></tr><tr><td>eval/samples_per_second</td><td>█▅▃▃▆▅▆▄▃▂▇▄▄▄▁</td></tr><tr><td>eval/steps_per_second</td><td>█▅▃▃▆▅▆▄▃▂▇▄▄▄▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▆▂▅▃▁▃▅█▇▆▂▃▃▄▂</td></tr><tr><td>train/learning_rate</td><td>█▇▇▇▆▅▅▅▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▂▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.91526</td></tr><tr><td>eval/loss</td><td>0.25948</td></tr><tr><td>eval/runtime</td><td>17.8854</td></tr><tr><td>eval/samples_per_second</td><td>212.464</td></tr><tr><td>eval/steps_per_second</td><td>26.558</td></tr><tr><td>total_flos</td><td>2.044809537754675e+16</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>7500</td></tr><tr><td>train/grad_norm</td><td>2.20457</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.26</td></tr><tr><td>train_loss</td><td>0.30279</td></tr><tr><td>train_runtime</td><td>2096.6224</td></tr><tr><td>train_samples_per_second</td><td>57.235</td></tr><tr><td>train_steps_per_second</td><td>3.577</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LoRA_2_4</strong> at: <a href='https://wandb.ai/hpml99/lora_and_qlora_v2/runs/3t9v2yy4' target=\"_blank\">https://wandb.ai/hpml99/lora_and_qlora_v2/runs/3t9v2yy4</a><br/> View project at: <a href='https://wandb.ai/hpml99/lora_and_qlora_v2' target=\"_blank\">https://wandb.ai/hpml99/lora_and_qlora_v2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240512_005032-3t9v2yy4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bd1a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3acadffc-ed7b-4e22-9677-957315157522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pynvml import *\n",
    "\n",
    "\n",
    "# def print_gpu_utilization():\n",
    "#     nvmlInit()\n",
    "#     handle = nvmlDeviceGetHandleByIndex(0)\n",
    "#     info = nvmlDeviceGetMemoryInfo(handle)\n",
    "#     print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "# def print_summary(result):\n",
    "#     print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "#     print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "#     print_gpu_utilization()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b1c8602-b146-4abe-83f5-98438ec1e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 10652 MB.\n",
      "Time: 2096.62\n",
      "Samples/second: 57.23\n",
      "GPU memory occupied: 10652 MB.\n"
     ]
    }
   ],
   "source": [
    "# Print GPU Memory utilization\n",
    "print_gpu_utilization()\n",
    "\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ee95413-ce80-4bfc-b712-5d3c975ac6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# q_peft_model_name = 'roberta-base-peft-8bit'\n",
    "# model.save_pretrained(q_peft_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038198cf-0953-47e7-bd47-b073d05f8378",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "It's time to have some fun putting our model to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f88ad420-3f46-4eff-9d71-0ce388163062",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify(text):\n",
    "  inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "  output = model(**inputs)\n",
    "\n",
    "  prediction = output.logits.argmax(dim=-1).item()\n",
    "\n",
    "  print(f'\\n Class: {prediction}, Label: {id2label[prediction]}, Text: {text}')\n",
    "  # return id2label[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc52bb94-5e13-4943-9225-a6d7fd053579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify( \"Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\")\n",
    "# classify( \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183be7e-514f-4e64-a6f4-314a827e6be5",
   "metadata": {},
   "source": [
    "### Evaluate Models\n",
    "\n",
    "To measure the improvement of the Training process we will need a baseline; let's compare the trained models with an untrained one.\n",
    "\n",
    "Take a Look how the base model performs vs finetuned one\n",
    "\n",
    "1.   The trained models against the untrained one\n",
    "2.   The PEFT Model vs the regular fine-tuned one  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9054fed4-564c-4602-8594-0db4cecbe3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def evaluate_model(inference_model, dataset):\n",
    "\n",
    "    eval_dataloader = DataLoader(dataset.rename_column(\"label\", \"labels\"), batch_size=8, collate_fn=data_collator)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        predictions, references = predictions, batch[\"labels\"]\n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    print(eval_metric)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "775e6438-b813-4b18-8931-96af78ca1302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 475/475 [00:16<00:00, 29.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.19921052631578948}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the non fine-tuned model\n",
    "evaluate_model(AutoModelForSequenceClassification.from_pretrained(base_model, id2label=id2label), test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3cef39c-5f72-4e87-8dad-eebcb6e4b49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 475/475 [00:17<00:00, 27.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9226315789473685}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the PEFT fine-tuned model\n",
    "evaluate_model(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086e781-a554-4822-983b-8d38161eff7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
